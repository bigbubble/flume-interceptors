agent1.sources = r1
agent1.channels = c1
agent1.sinks = s1

# For each one of the sources, the type is defined
agent1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource

# The channel can be defined as follows.
agent1.sources.r1.channels = c1
agent1.sources.r1.batchSize=5000
agent1.sources.r1.batchDurationMillis=5000
agent1.sources.r1.kafka.bootstrap.servers = 192.168.1.1:9092,192.168.1.2:9092,192.168.1.3:9092
agent1.sources.r1.kafka.topics = test*
agent1.sources.r1.setTopicHeader=true
agent1.sources.r1.topicHeader=topic


agent1.sources.r1.interceptors=i1
agent1.sources.r1.interceptors.i1.type=me.pengbo.flume.interceptors.ExtractLogTimestampAndKafkaTopicToHeaderInterceptor$ExtractLogTimestampAndKafkaTopicToHeaderInterceptorBuilder
agent1.sources.r1.interceptors.i1.formatter=yyyy-MM-dd HH:mm:ss.SSS
agent1.sources.r1.interceptors.i1.extract=\\[(\\d\\d\\dd\\dd-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d.\\d\\d\\d)\\]
agent1.sources.r1.interceptors.i1.hadoopDir=topicAndPartitionerId
#必须和agent1.sources.r1.setTopicHeader一致
agent1.sources.r1.interceptors.i1.setTopicHeader=true
#必须和agent1.sources.r1.topicHeader一致
agent1.sources.r1.interceptors.i1.topicHeader=topic

# Each sink's type must be defined
agent1.sinks.s1.type = hdfs
agent1.sinks.s1.hdfs.path=hdfs://192.168.3.107:9000/%{topicAndPartitionerId}/%y-%m-%d/%H/%M
agent1.sinks.s1.hdfs.filePrefix=test
agent1.sinks.s1.hdfs.batchSize=5000
agent1.sinks.s1.hdfs.fileType=DataStream


#Specify the channel the sink should use
agent1.sinks.s1.channel = c1

# Each channel's type is defined.
agent1.channels.c1.type = memory


# Other config values specific to each type of channel(sink or source)
# can be defined as well
# In this case, it specifies the capacity of the memory channel
agent1.channels.c1.capacity = 100